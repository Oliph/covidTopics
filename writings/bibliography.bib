
@inproceedings{abelMethodologicalFrameworkDictionary2019,
  title = {A {{Methodological Framework}} for {{Dictionary}} and {{Rule}}-Based {{Text Classification}}:},
  shorttitle = {A {{Methodological Framework}} for {{Dictionary}} and {{Rule}}-Based {{Text Classification}}},
  booktitle = {Proceedings of the 11th {{International Joint Conference}} on {{Knowledge Discovery}}, {{Knowledge Engineering}} and {{Knowledge Management}}},
  author = {Abel, Jennifer and Lantow, Birger},
  year = {2019},
  pages = {330--337},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Vienna, Austria}},
  doi = {10.5220/0008121503300337},
  file = {/home/olivier/nextcloud/zotero/Abel and Lantow - 2019 - A Methodological Framework for Dictionary and Rule.pdf},
  isbn = {978-989-758-382-7},
  language = {en}
}

@inproceedings{aharoniUnsupervisedDomainClusters2020,
  title = {Unsupervised {{Domain Clusters}} in {{Pretrained Language Models}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Aharoni, Roee and Goldberg, Yoav},
  year = {2020},
  pages = {7747--7763},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.692},
  file = {/home/olivier/nextcloud/zotero/Aharoni and Goldberg - 2020 - Unsupervised Domain Clusters in Pretrained Languag.pdf},
  language = {en}
}

@inproceedings{allaouiConsiderablyImprovingClustering2020,
  title = {Considerably {{Improving Clustering Algorithms Using UMAP Dimensionality Reduction Technique}}: {{A Comparative Study}}},
  shorttitle = {Considerably {{Improving Clustering Algorithms Using UMAP Dimensionality Reduction Technique}}},
  booktitle = {Image and {{Signal Processing}}},
  author = {Allaoui, Mebarka and Kherfi, Mohammed Lamine and Cheriet, Abdelhakim},
  editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin and Nouboud, Fathallah},
  year = {2020},
  pages = {317--325},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-51935-3_34},
  abstract = {Dimensionality reduction is widely used in machine learning and big data analytics since it helps to analyze and to visualize large, high-dimensional datasets. In particular, it can considerably help to perform tasks like data clustering and classification. Recently, embedding methods have emerged as a promising direction for improving clustering accuracy. They can preserve the local structure and simultaneously reveal the global structure of data, thereby reasonably improving clustering performance. In this paper, we investigate how to improve the performance of several clustering algorithms using one of the most successful embedding techniques: Uniform Manifold Approximation and Projection or UMAP. This technique has recently been proposed as a manifold learning technique for dimensionality reduction. It is based on Riemannian geometry and algebraic topology. Our main hypothesis is that UMAP would permit to find the best clusterable embedding manifold, and therefore, we applied it as a preprocessing step before performing clustering. We compare the results of many well-known clustering algorithms such ask-means, HDBSCAN, GMM and Agglomerative Hierarchical Clustering when they operate on the low-dimension feature space yielded by UMAP. A series of experiments on several image datasets demonstrate that the proposed method allows each of the clustering algorithms studied to improve its performance on each dataset considered. Based on Accuracy measure, the improvement can reach a remarkable rate of 60\%.},
  file = {/home/olivier/nextcloud/zotero/Allaoui et al. - 2020 - Considerably Improving Clustering Algorithms Using.pdf},
  isbn = {978-3-030-51935-3},
  keywords = {Big data analytics,Clustering,Comparative study,Dimensionality reduction,Embedding manifold,Machine learning,UMAP},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{angelovTop2VecDistributedRepresentations2020a,
  ids = {angelovTop2VecDistributedRepresentations2020},
  title = {{{Top2Vec}}: {{Distributed Representations}} of {{Topics}}},
  shorttitle = {{{Top2Vec}}},
  author = {Angelov, Dimo},
  year = {2020},
  month = aug,
  abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present top2vec, which leverages joint document and word semantic embedding to find topic vectors. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that top2vec finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
  archivePrefix = {arXiv},
  eprint = {2008.09470},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Angelov - 2020 - Top2Vec Distributed Representations of Topics.pdf},
  journal = {arXiv:2008.09470 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ankerstOPTICSOrderingPoints1999,
  title = {{{OPTICS}}: Ordering Points to Identify the Clustering Structure},
  author = {Ankerst, Mihael and Breunig, Markus M and Kriegel, Hans-Peter and Sander, J{\"o}rg},
  year = {1999},
  volume = {28},
  pages = {49--60},
  publisher = {{ACM New York, NY, USA}},
  file = {/home/olivier/nextcloud/zotero/Ankerst et al. - 1999 - OPTICS ordering points to identify the clustering.pdf},
  journal = {ACM Sigmod record},
  number = {2}
}

@article{asgari-chenaghluTopicBERTTransformerTransfer2020,
  title = {{{TopicBERT}}: {{A Transformer}} Transfer Learning Based Memory-Graph Approach for Multimodal Streaming Social Media Topic Detection},
  shorttitle = {{{TopicBERT}}},
  author = {{Asgari-Chenaghlu}, Meysam and {Feizi-Derakhshi}, Mohammad-Reza and {farzinvash}, Leili and Balafar, Mohammad-Ali and Motamed, Cina},
  year = {2020},
  month = aug,
  abstract = {Real time nature of social networks with bursty short messages and their respective large data scale spread among vast variety of topics are research interest of many researchers. These properties of social networks which are known as 5'Vs of big data has led to many unique and enlightenment algorithms and techniques applied to large social networking datasets and data streams. Many of these researches are based on detection and tracking of hot topics and trending social media events that help revealing many unanswered questions. These algorithms and in some cases software products mostly rely on the nature of the language itself. Although, other techniques such as unsupervised data mining methods are language independent but many requirements for a comprehensive solution are not met. Many research issues such as noisy sentences that adverse grammar and new online user invented words are challenging maintenance of a good social network topic detection and tracking methodology; The semantic relationship between words and in most cases, synonyms are also ignored by many of these researches. In this research, we use Transformers combined with an incremental community detection algorithm. Transformer in one hand, provides the semantic relation between words in different contexts. On the other hand, the proposed graph mining technique enhances the resulting topics with aid of simple structural rules. Named entity recognition from multimodal data, image and text, labels the named entities with entity type and the extracted topics are tuned using them. All operations of proposed system has been applied with big social data perspective under NoSQL technologies. In order to present a working and systematic solution, we combined MongoDB with Neo4j as two major database systems of our work. The proposed system shows higher precision and recall compared to other methods in three different datasets.},
  archivePrefix = {arXiv},
  eprint = {2008.06877},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Asgari-Chenaghlu et al. - 2020 - TopicBERT A Transformer transfer learning based m.pdf},
  journal = {arXiv:2008.06877 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{basileSemEval2019TaskMultilingual2019,
  title = {{{SemEval}}-2019 {{Task}} 5: {{Multilingual Detection}} of {{Hate Speech Against Immigrants}} and {{Women}} in {{Twitter}}},
  shorttitle = {{{SemEval}}-2019 {{Task}} 5},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Basile, Valerio and Bosco, Cristina and Fersini, Elisabetta and Nozza, Debora and Patti, Viviana and Rangel Pardo, Francisco Manuel and Rosso, Paolo and Sanguinetti, Manuela},
  year = {2019},
  pages = {54--63},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota, USA}},
  doi = {10.18653/v1/S19-2007},
  abstract = {The paper describes the organization of the SemEval 2019 Task 5 about the detection of hate speech against immigrants and women in Spanish and English messages extracted from Twitter. The task is organized in two related classification subtasks: a main binary subtask for detecting the presence of hate speech, and a finer-grained one devoted to identifying further features in hateful contents such as the aggressive attitude and the target harassed, to distinguish if the incitement is against an individual rather than a group. HatEval has been one of the most popular tasks in SemEval-2019 with a total of 108 submitted runs for Subtask A and 70 runs for Subtask B, from a total of 74 different teams. Data provided for the task are described by showing how they have been collected and annotated. Moreover, the paper provides an analysis and discussion about the participant systems and the results they achieved in both subtasks.},
  file = {/home/olivier/Zotero/storage/ILR8JDTA/Basile et al. - 2019 - SemEval-2019 Task 5 Multilingual Detection of Hat.pdf},
  language = {en}
}

@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M},
  year = {2003},
  volume = {3},
  pages = {993--1022},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  file = {/home/olivier/nextcloud/zotero/Blei - 2003 - Latent Dirichlet Allocation.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archivePrefix = {arXiv},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf},
  journal = {arXiv:1607.04606 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{bojkovskySTUFIITSemEval2019Task2019,
  title = {{{STUFIIT}} at {{SemEval}}-2019 {{Task}} 5: {{Multilingual Hate Speech Detection}} on {{Twitter}} with {{MUSE}} and {{ELMo Embeddings}}},
  shorttitle = {{{STUFIIT}} at {{SemEval}}-2019 {{Task}} 5},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Bojkovsk{\'y}, Michal and Pikuliak, Mat{\'u}{\v s}},
  year = {2019},
  pages = {464--468},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota, USA}},
  doi = {10.18653/v1/S19-2082},
  abstract = {We evaluate the viability of multilingual learning for the task of hate speech detection. We also experiment with adversarial learning as a means of creating a multilingual model. Ultimately our multilingual models have had worse results than their monolignual counterparts. We find that the choice of word representations (word embeddings) is very crucial for deep learning as a simple switch between MUSE and ELMo embeddings has shown a 34\% increase in accuracy. This also shows the importance of context when dealing with online content.},
  file = {/home/olivier/nextcloud/zotero/Bojkovský and Pikuliak - 2019 - STUFIIT at SemEval-2019 Task 5 Multilingual Hate .pdf},
  language = {en}
}

@article{brunsQuantitativeApproachesComparing2012,
  title = {Quantitative {{Approaches}} to {{Comparing Communication Patterns}} on {{Twitter}}},
  author = {Bruns, Axel and Stieglitz, Stefan},
  year = {2012},
  month = jul,
  volume = {30},
  pages = {160--185},
  issn = {1522-8835, 1522-8991},
  doi = {10.1080/15228835.2012.744249},
  abstract = {To date, available literature mainly discusses Twitter activity patterns in the context of individual case studies, while comparative research on a large number of communicative events, their dynamics and patterns is missing. By conducting a comparative study of more than forty different cases (covering topics such as elections, natural disasters, corporate crises, and televised events) we identify a number of distinct types of discussion which can be observed on Twitter. Drawing on a range of communicative metrics, we show that thematic and contextual factors influence the usage of different communicative tools available to Twitter users, such as original tweets, @replies, retweets, and URLs. Based on this first analysis of the overall metrics of Twitter discussions, we also demonstrate stable patterns in the use of Twitter in the context of major topics and events.},
  file = {/home/olivier/nextcloud/zotero/Bruns and Stieglitz - 2012 - Quantitative Approaches to Comparing Communication.pdf},
  journal = {Journal of Technology in Human Services},
  language = {en},
  number = {3-4}
}

@inproceedings{caneteSpanishPreTrainedBERT2020,
  title = {Spanish {{Pre}}-{{Trained BERT Model}} and {{Evaluation Data}}},
  booktitle = {To Appear in {{PML4DC}} at {{ICLR}} 2020},
  author = {Ca{\~n}ete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and P{\'e}rez, Jorge},
  year = {2020}
}

@misc{ConnectingStreamingEndpoint,
  title = {Connecting to a Streaming Endpoint},
  file = {/home/olivier/Zotero/storage/CR6FQPUQ/connecting.html},
  howpublished = {https://developer.twitter.com/en/docs/twitter-api/v1/tweets/filter-realtime/guides/connecting},
  language = {en}
}

@article{curiskisEvaluationDocumentClustering2020,
  title = {An Evaluation of Document Clustering and Topic Modelling in Two Online Social Networks: {{Twitter}} and {{Reddit}}},
  shorttitle = {An Evaluation of Document Clustering and Topic Modelling in Two Online Social Networks},
  author = {Curiskis, Stephan A. and Drake, Barry and Osborn, Thomas R. and Kennedy, Paul J.},
  year = {2020},
  month = mar,
  volume = {57},
  pages = {102034},
  issn = {03064573},
  doi = {10.1016/j.ipm.2019.04.002},
  abstract = {Methods for document clustering and topic modelling in online social networks (OSNs) offer a means of categorising, annotating and making sense of large volumes of user generated content. Many techniques have been developed over the years, ranging from text mining and clustering methods to latent topic models and neural embedding approaches. However, many of these methods deliver poor results when applied to OSN data as such text is notoriously short and noisy, and often results are not comparable across studies. In this study we evaluate several techniques for document clustering and topic modelling on three datasets from Twitter and Reddit. We benchmark four different feature representations derived from term-frequency inverse-document-frequency (tf-idf) matrices and word embedding models combined with four clustering methods, and we include a Latent Dirichlet Allocation topic model for comparison. Several different evaluation measures are used in the literature, so we provide a discussion and recommendation for the most appropriate extrinsic measures for this task. We also demonstrate the performance of the methods over data sets with different document lengths. Our results show that clustering techniques applied to neural embedding feature representations delivered the best performance over all data sets using appropriate extrinsic evaluation measures. We also demonstrate a method for interpreting the clusters with a top-words based approach using tf-idf weights combined with embedding distance measures.},
  file = {/home/olivier/nextcloud/zotero/Curiskis et al. - 2020 - An evaluation of document clustering and topic mod.pdf},
  journal = {Information Processing \& Management},
  keywords = {datapop,methods,nlp,topic detection,TOREAD,twitter},
  language = {en},
  number = {2}
}

@inproceedings{dai2018multilingual,
  ids = {daiMultilingualWordEmbedding},
  title = {Multilingual Word Embedding for Zero-Shot Text Classification},
  booktitle = {Annual Meeting of the American Political Science Association, Boston},
  author = {Dai, Yaoyao and Radford, Benjamin},
  year = {2018},
  file = {/home/olivier/nextcloud/zotero/Dai - Multilingual Word Embedding for Zero-Shot Text Cla.pdf}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{eisenschlosMultiFiTEfficientMultilingual2019,
  title = {{{MultiFiT}}: {{Efficient Multi}}-Lingual {{Language Model Fine}}-Tuning},
  shorttitle = {{{MultiFiT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Eisenschlos, Julian and Ruder, Sebastian and Czapla, Piotr and Kadras, Marcin and Gugger, Sylvain and Howard, Jeremy},
  year = {2019},
  pages = {5701--5706},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1572},
  abstract = {Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code1.},
  file = {/home/olivier/nextcloud/zotero/Eisenschlos et al. - 2019 - MultiFiT Efficient Multi-lingual Language Model F.pdf},
  language = {en}
}

@article{eisensteinUnsupervisedLearningLexiconBased,
  title = {Unsupervised {{Learning}} for {{Lexicon}}-{{Based Classification}}},
  author = {Eisenstein, Jacob},
  pages = {7},
  abstract = {In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics.},
  file = {/home/olivier/nextcloud/zotero/Eisenstein - Unsupervised Learning for Lexicon-Based Classifica.pdf},
  language = {en}
}

@article{esterDensityBasedAlgorithmDiscovering1996,
  title = {A {{Density}}-{{Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  year = {1996},
  pages = {6},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  file = {/home/olivier/Zotero/storage/JSA2QP53/Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf},
  language = {en}
}

@inproceedings{fankhauser-etal2016,
  title = {Topical Diversification over Time in the {{Royal Society Corpus}}},
  booktitle = {Proceedings of {{DH}} 2016},
  author = {Fankhauser, Peter and Knappen, J{\"o}rg and Teich, Elke},
  year = {2016},
  month = jul,
  address = {{Krakow, Poland}}
}

@article{fortunaSurveyAutomaticDetection2018,
  title = {A {{Survey}} on {{Automatic Detection}} of {{Hate Speech}} in {{Text}}},
  author = {Fortuna, Paula and Nunes, S{\'e}rgio},
  year = {2018},
  month = sep,
  volume = {51},
  pages = {1--30},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3232676},
  file = {/home/olivier/Zotero/storage/A7WF5JT4/Fortuna and Nunes - 2018 - A Survey on Automatic Detection of Hate Speech in .pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {4}
}

@incollection{franciscusWordMoverDistance2019,
  title = {Word {{Mover}}'s {{Distance}} for {{Agglomerative Short Text Clustering}}},
  booktitle = {Intelligent {{Information}} and {{Database Systems}}},
  author = {Franciscus, Nigel and Ren, Xuguang and Wang, Junhu and Stantic, Bela},
  editor = {Nguyen, Ngoc Thanh and Gaol, Ford Lumban and Hong, Tzung-Pei and Trawi{\'n}ski, Bogdan},
  year = {2019},
  volume = {11431},
  pages = {128--139},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-14799-0_11},
  abstract = {In the era of information overload, text clustering plays an important part in the analysis processing pipeline. Partitioning highquality texts into unseen categories tremendously helps applications in information retrieval, databases, and business intelligence domains. Short texts from social media environment such as tweets, however, remain difficult to interpret due to the broad aspects of contexts. Traditional text similarity approaches only rely on the lexical matching while ignoring the semantic meaning of words. Recent advances in distributional semantic space have opened an alternative approach in utilizing high-quality word embeddings to aid the interpretation of text semantics. In this paper, we investigate the word mover's distance metrics to automatically cluster short text using the word semantic information. We utilize the agglomerative strategy as the clustering method to efficiently group texts based on their similarity. The experiment indicates the word mover's distance outperformed other standard metrics in the short text clustering task.},
  file = {/home/olivier/nextcloud/zotero/Franciscus et al. - 2019 - Word Mover’s Distance for Agglomerative Short Text.pdf},
  isbn = {978-3-030-14798-3 978-3-030-14799-0},
  language = {en}
}

@article{gaoGenerationTopicEvolution2020,
  title = {Generation of Topic Evolution Graphs from Short Text Streams},
  author = {Gao, Wang and Peng, Min and Wang, Hua and Zhang, Yanchun and Han, Weiguang and Hu, Gang and Xie, Qianqian},
  year = {2020},
  month = mar,
  volume = {383},
  pages = {282--294},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.11.077},
  abstract = {Topic evolution mining on short texts is an important research topic in natural language processing. Existing methods have been focused either on the topic evolution of normal documents or on the evolution of topics along a timeline. In this paper, we aim to generate topic evolutionary graphs from short texts, which not only capture the main topic timeline, but also reveal the correlations between related subtopics. Firstly, we propose an Encoder-only Transformer Language Model (ETLM) to quantify the relationship between words. Then we propose a novel topic model, referred as weighted Conditional random field regularized Correlated Topic Model (CCTM), which leverages semantic correlations to discover meaningful topics and topic correlations. Finally, topic evolutionary graphs are generated by an Online version of CCTM (OCCTM) to capture the evolutionary patterns of main topics and related subtopics. Experimental results on real-world datasets demonstrate our method outperforms baselines on quality of topics and presents motivated patterns for topic evolution mining.},
  file = {/home/olivier/Zotero/storage/N8SPEFXN/Gao et al. - 2020 - Generation of topic evolution graphs from short te.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@inproceedings{guptaBetterWordEmbeddings2019,
  title = {Better {{Word Embeddings}} by {{Disentangling Contextual}} N-{{Gram Information}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Gupta, Prakhar and Pagliardini, Matteo and Jaggi, Martin},
  year = {2019},
  pages = {933--939},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1098},
  abstract = {Pre-trained word vectors are ubiquitous in Natural Language Processing applications. In this paper, we show how training word embeddings jointly with bigram and even trigram embeddings, results in improved unigram embeddings. We claim that training word embeddings along with higher n-gram embeddings helps in the removal of the contextual information from the unigrams, resulting in better stand-alone word embeddings. We empirically show the validity of our hypothesis by outperforming other competing word representation models by a significant margin on a wide variety of tasks. We make our models publicly available.},
  file = {/home/olivier/nextcloud/zotero/Gupta et al. - 2019 - Better Word Embeddings by Disentangling Contextual.pdf},
  language = {en}
}

@article{hartmannComparingAutomatedText2019,
  title = {Comparing Automated Text Classification Methods},
  author = {Hartmann, Jochen and Huppertz, Juliana and Schamp, Christina and Heitmann, Mark},
  year = {2019},
  month = mar,
  volume = {36},
  pages = {20--38},
  issn = {01678116},
  doi = {10.1016/j.ijresmar.2018.09.009},
  abstract = {Online social media drive the growth of unstructured text data. Many marketing applications require structuring this data at scales non-accessible to human coding, e.g., to detect communication shifts in sentiment or other researcher-defined content categories. Several methods have been proposed to automatically classify unstructured text. This paper compares the performance of ten such approaches (five lexicon-based, five machine learning algorithms) across 41 social media datasets covering major social media platforms, various sample sizes, and languages. So far, marketing research relies predominantly on support vector machines (SVM) and Linguistic Inquiry and Word Count (LIWC). Across all tasks we study, either random forest (RF) or naive Bayes (NB) performs best in terms of correctly uncovering human intuition. In particular, RF exhibits consistently high performance for three-class sentiment, NB for small samples sizes. SVM never outperform the remaining methods. All lexicon-based approaches, LIWC in particular, perform poorly compared with machine learning. In some applications, accuracies only slightly exceed chance. Since additional considerations of text classification choice are also in favor of NB and RF, our results suggest that marketing research can benefit from considering these alternatives.},
  file = {/home/olivier/Zotero/storage/VV7DIGYQ/Hartmann et al. - 2019 - Comparing automated text classification methods.pdf},
  journal = {International Journal of Research in Marketing},
  language = {en},
  number = {1}
}

@article{hennigWhatAreTrue2015,
  title = {What Are the True Clusters?},
  author = {Hennig, Christian},
  year = {2015},
  month = feb,
  abstract = {Constructivist philosophy and Hasok Chang's active scientific realism are used to argue that the idea of ``truth'' in cluster analysis depends on the context and the clustering aims. Different characteristics of clusterings are required in different situations. Researchers should be explicit about on what requirements and what idea of ``true clusters'' their research is based, because clustering becomes scientific not through uniqueness but through transparent and open communication. The idea of ``natural kinds'' is a human construct, but it highlights the human experience that the reality outside the observer's control seems to make certain distinctions between categories inevitable.},
  archivePrefix = {arXiv},
  eprint = {1502.02555},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Hennig - 2015 - What are the true clusters.pdf},
  journal = {arXiv:1502.02555 [stat]},
  keywords = {03A05; 62H30; 91C20,Statistics - Other Statistics},
  language = {en},
  primaryClass = {stat}
}

@article{jagtapHIERARCHICALTEXTCLASSIFICATION2018,
  title = {{{HIERARCHICAL TEXT CLASSIFICATION USING DICTIONARY}}-{{BASED APPROACH AND CONVOLUTIONAL NEURAL NETWORK}}},
  author = {Jagtap, Sneha N and Adamuthe, Amol C},
  year = {2018},
  pages = {7},
  abstract = {As increasing amount of electronic information which is usually in textual form. There is an important requirement to classify textual reports generated by organizations into various categories. Machine Learning techniques can be employed to classify text autonomously in an effective manner. We propose a generic web-based application for classifying text using machine learning techniques. For creating a baseline implementation, we projected a dictionary-based approach for text classification. The dictionary is populated based on domain-specific inputs. The dictionary is expanded using automated techniques. This helps us in getting an end-to-end implementation of full pipeline. In subsequent phases, we proposed our approach with Convolutional Neural Network (CNN) for text classification.},
  file = {/home/olivier/nextcloud/zotero/Jagtap and Adamuthe - 2018 - HIERARCHICAL TEXT CLASSIFICATION USING DICTIONARY-.pdf},
  language = {en},
  number = {2581}
}

@article{joulin2016bag,
  title = {Bag of Tricks for Efficient Text Classification},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  year = {2016},
  archivePrefix = {arXiv},
  eprint = {1607.01759},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1607.01759}
}

@article{joulin2016fasttext,
  title = {{{FastText}}.Zip: {{Compressing}} Text Classification Models},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  year = {2016},
  archivePrefix = {arXiv},
  eprint = {1612.03651},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1612.03651}
}

@article{li2017data,
  ids = {liDataSetsWord},
  title = {Data Sets: {{Word}} Embeddings Learned from Tweets and General Data},
  author = {Li, Quanzhi and Shah, Sameena and Liu, Xiaomo and Nourbakhsh, Armineh},
  year = {2017},
  archivePrefix = {arXiv},
  eprint = {1708.03994},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Li et al. - 2017 - Data sets Word embeddings learned from tweets and.pdf},
  primaryClass = {cs.CL}
}

@article{liComparisonWordEmbeddings2018,
  title = {Comparison of {{Word Embeddings}} and {{Sentence Encodings}} as {{Generalized Representations}} for {{Crisis Tweet Classification Tasks}}},
  author = {Li, Hongmin and Caragea, Doina and Li, Xukun and Caragea, Cornelia},
  year = {2018},
  pages = {13},
  abstract = {Many machine learning and natural language processing approaches, including supervised and domain adaptation algorithms, have been proposed and studied in the context of filtering crisis tweets. However, the application of these approaches in practice is still challenging due to the time-critical requirements of emergency response operations, and also to the diversity and unique characteristics of emergency events. To address this limitation, we explore the idea of building ``generalized'' classifiers for filtering crisis tweets, classifiers which can be pre-trained and ready to use in real-time, while they generalize well on tweets from future disasters. We propose to achieve this objective using a simple feature-based adaptation approach, where tweets are represented as dense numeric vectors of reduced dimensionality using either word embeddings or sentence encodings. Given that several types of word embeddings and sentence encodings exist, we compare tweet representations corresponding to different word embeddings and sentence encodings with the goal of understanding what embeddings/encodings are more suitable for use in crisis tweet classification tasks. Our experimental results on three crisis tweet classification tasks suggest that the tweet representations based on GloVe embeddings produce better results than the representations that use other embeddings, when employed with traditional supervised learning algorithms. Furthermore, the GloVe embeddings trained on crisis data produce better results on more specific crisis tweet classification tasks (e.g., tweets informative versus non-informative), while the GloVe embeddings pre-trained on a large collection of general tweets produce better results on more general classification tasks (tweets relevant or not relevant to a crisis).},
  file = {/home/olivier/nextcloud/zotero/Li et al. - 2018 - Comparison of Word Embeddings and Sentence Encodin.pdf},
  journal = {New Zealand},
  language = {en}
}

@article{ljungberg2017dimensionality,
  ids = {ljungbergDimensionalityReductionBagofwords},
  title = {Dimensionality Reduction for Bag-of-Words Models: {{PCA}} vs {{LSA}}},
  author = {Ljungberg, Benjamin Fayyazuddin},
  year = {2017},
  file = {/home/olivier/Zotero/storage/N3NWAII3/Ljungberg - Dimensionality reduction for bag-of-words models .pdf},
  journal = {Semanticscholar. org}
}

@inproceedings{mcinnesAcceleratedHierarchicalDensity2017,
  title = {Accelerated {{Hierarchical Density Based Clustering}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {McInnes, Leland and Healy, John},
  year = {2017},
  month = nov,
  pages = {33--42},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2017.12},
  abstract = {We present an accelerated algorithm for hierarchical density based clustering. Our new algorithm improves upon HDBSCAN*, which itself provided a significant qualitative improvement over the popular DBSCAN algorithm. The accelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while supporting variable density clusters, and eliminating the need for the difficult to tune distance scale parameter epsilon. This makes accelerated HDBSCAN* the default choice for density based clustering.},
  file = {/home/olivier/Zotero/storage/A7MW9UGS/8215642.html},
  keywords = {accelerated algorithm,accelerated HDBSCAN* algorithm,accelerated hierarchical density based clustering,Acceleration,Algorithm design and analysis,clustering,Clustering algorithms,Couplings,data analysis,Data analysis,DBSCAN algorithm,density based clustering,Density functional theory,distance scale parameter tuning,hierarchical clustering,pattern clustering,Robustness,significant qualitative improvement,variable density clusters}
}

@article{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. e result is a practical scalable algorithm that is applicable to real world data. e UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archivePrefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  file = {/home/olivier/Zotero/storage/VXM2MIKJ/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf},
  journal = {arXiv:1802.03426 [cs, stat]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{mengDiscriminativeTopicMining2020,
  title = {Discriminative {{Topic Mining}} via {{Category}}-{{Name Guided Text Embedding}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Wang, Zihan and Zhang, Chao and Zhang, Yu and Han, Jiawei},
  year = {2020},
  month = apr,
  pages = {2121--2132},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3366423.3380278},
  abstract = {Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users' particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines highquality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.},
  file = {/home/olivier/nextcloud/zotero/Meng et al. - 2020 - Discriminative Topic Mining via Category-Name Guid.pdf},
  isbn = {978-1-4503-7023-3},
  language = {en}
}

@article{mengWeaklySupervisedHierarchicalText2019,
  title = {Weakly-{{Supervised Hierarchical Text Classification}}},
  author = {Meng, Yu and Shen, Jiaming and Zhang, Chao and Han, Jiawei},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {6826--6833},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33016826},
  abstract = {Hierarchical text classification, which aims to classify text documents into a given hierarchy, is an important task in many real-world applications. Recently, deep neural models are gaining increasing popularity for text classification due to their expressive power and minimum requirement for feature engineering. However, applying deep neural networks for hierarchical text classification remains challenging, because they heavily rely on a large amount of training data and meanwhile cannot easily determine appropriate levels of documents in the hierarchical setting. In this paper, we propose a weakly-supervised neural method for hierarchical text classification. Our method does not require a large amount of training data but requires only easy-to-provide weak supervision signals such as a few class-related documents or keywords. Our method effectively leverages such weak supervision signals to generate pseudo documents for model pre-training, and then performs self-training on real unlabeled data to iteratively refine the model. During the training process, our model features a hierarchical neural structure, which mimics the given hierarchy and is capable of determining the proper levels for documents with a blocking mechanism. Experiments on three datasets from different domains demonstrate the efficacy of our method compared with a comprehensive set of baselines.},
  file = {/home/olivier/nextcloud/zotero/Meng et al. - 2019 - Weakly-Supervised Hierarchical Text Classification.pdf},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  language = {en}
}

@article{minModelingNonnegativeData,
  title = {Modeling {{Nonnegative Data}} with {{Clumping}} at {{Zero}}: {{A Survey}}},
  author = {Min, Yongyi and Agresti, Alan},
  pages = {27},
  abstract = {Applications in which data take nonnegative values but have a substantial proportion of values at zero occur in many disciplines. The modeling of such ``clumped-at-zero'' or ``zero-inflated'' data is challenging. We survey models that have been proposed. We consider cases in which the response for the non-zero observations is continuous and in which it is discrete. For the continuous and then the discrete case, we review models for analyzing cross-sectional data. We then summarize extensions for repeated measurement analyses (e.g., in longitudinal studies), for which the literature is still sparse. We also mention applications in which more than one clump can occur and we suggest problems for future research.},
  file = {/home/olivier/nextcloud/zotero/Min and Agresti - Modeling Nonnegative Data with Clumping at Zero A.pdf},
  language = {en}
}

@article{minSurveyClusteringDeep2018,
  title = {A {{Survey}} of {{Clustering With Deep Learning}}: {{From}} the {{Perspective}} of {{Network Architecture}}},
  shorttitle = {A {{Survey}} of {{Clustering With Deep Learning}}},
  author = {Min, Erxue and Guo, Xifeng and Liu, Qiang and Zhang, Gen and Cui, Jianjing and Long, Jun},
  year = {2018},
  volume = {6},
  pages = {39501--39514},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2855437},
  abstract = {Clustering is a fundamental problem in many data-driven application domains, and clustering performance highly depends on the quality of data representation. Hence, linear or non-linear feature transformations have been extensively used to learn a better data representation for clustering. In recent years, a lot of works focused on using deep neural networks to learn a clustering-friendly representation, resulting in a significant increase of clustering performance. In this paper, we give a systematic survey of clustering with deep learning in views of architecture. Specifically, we first introduce the preliminary knowledge for better understanding of this field. Then, a taxonomy of clustering with deep learning is proposed and some representative methods are introduced. Finally, we propose some interesting future opportunities of clustering with deep learning and give some conclusion remarks.},
  file = {/home/olivier/nextcloud/zotero/Min et al. - 2018 - A Survey of Clustering With Deep Learning From th.pdf},
  journal = {IEEE Access},
  language = {en}
}

@inproceedings{moulaviDensityBasedClusteringValidation2014,
  title = {Density-{{Based Clustering Validation}}},
  booktitle = {Proceedings of the 2014 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Moulavi, Davoud and Jaskowiak, Pablo A. and Campello, Ricardo J. G. B. and Zimek, Arthur and Sander, J{\"o}rg},
  year = {2014},
  month = apr,
  pages = {839--847},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973440.96},
  abstract = {One of the most challenging aspects of clustering is validation, which is the objective and quantitative assessment of clustering results. A number of different relative validity criteria have been proposed for the validation of globular, clusters. Not all data, however, are composed of globular clusters. Density-based clustering algorithms seek partitions with high density areas of points (clusters, not necessarily globular) separated by low density areas, possibly containing noise objects. In these cases relative validity indices proposed for globular cluster validation may fail. In this paper we propose a relative validation index for density-based, arbitrarily shaped clusters. The index assesses clustering quality based on the relative density connection between pairs of objects. Our index is formulated on the basis of a new kernel density function, which is used to compute the density of objects and to evaluate the within- and between-cluster density connectedness of clustering results. Experiments on synthetic and real world data show the effectiveness of our approach for the evaluation and selection of clustering algorithms and their respective appropriate parameters.},
  file = {/home/olivier/Zotero/storage/UPBBY2RV/Moulavi et al. - 2014 - Density-Based Clustering Validation.pdf},
  isbn = {978-1-61197-344-0},
  language = {en}
}

@article{mullerCOVIDTwitterBERTNaturalLanguage2020,
  title = {{{COVID}}-{{Twitter}}-{{BERT}}: {{A Natural Language Processing Model}} to {{Analyse COVID}}-19 {{Content}} on {{Twitter}}},
  shorttitle = {{{COVID}}-{{Twitter}}-{{BERT}}},
  author = {M{\"u}ller, Martin and Salath{\'e}, Marcel and Kummervold, Per E.},
  year = {2020},
  month = may,
  abstract = {In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10\textendash 30\% marginal improvement compared to its base model, BERT-LARGE, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular from social media.},
  archivePrefix = {arXiv},
  eprint = {2005.07503},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Müller et al. - 2020 - COVID-Twitter-BERT A Natural Language Processing .pdf},
  journal = {arXiv:2005.07503 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  language = {en},
  primaryClass = {cs}
}

@article{nugrohoSurveyRecentMethods2020,
  title = {A Survey of Recent Methods on Deriving Topics from {{Twitter}}: Algorithm to Evaluation},
  shorttitle = {A Survey of Recent Methods on Deriving Topics from {{Twitter}}},
  author = {Nugroho, Robertus and Paris, Cecile and Nepal, Surya and Yang, Jian and Zhao, Weiliang},
  year = {2020},
  month = jul,
  volume = {62},
  pages = {2485--2519},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-019-01429-z},
  abstract = {In recent years, studies related to topic derivation in Twitter have gained a lot of interest from businesses and academics. The interconnection between users and information has made social media, especially Twitter, an ultimate platform for propagation of information about events in real time. Many applications require topic derivation from this social media platform. These include, for example, disaster management, outbreak detection, situation awareness, surveillance, and market analysis. Deriving topics from Twitter is challenging due to the short content of the individual posts. The environment itself is also highly dynamic. This paper presents a review of recent methods proposed to derive topics from social media platform from algorithms to evaluations. With regard to algorithms, we classify them based on the features they exploit, such as content, social interactions, and temporal aspects. In terms of evaluations, we discuss the datasets and metrics generally used to evaluate the methods. Finally, we highlight the gaps in the research this far and the problems that remain to be addressed.},
  file = {/home/olivier/nextcloud/zotero/Nugroho et al. - 2020 - A survey of recent methods on deriving topics from.pdf},
  journal = {Knowledge and Information Systems},
  language = {en},
  number = {7}
}

@inproceedings{peineltTBERTTopicModels2020,
  title = {{{tBERT}}: {{Topic Models}} and {{BERT Joining Forces}} for {{Semantic Similarity Detection}}},
  shorttitle = {{{tBERT}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Peinelt, Nicole and Nguyen, Dong and Liakata, Maria},
  year = {2020},
  pages = {7047--7055},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.630},
  abstract = {Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.},
  file = {/home/olivier/nextcloud/zotero/Peinelt et al. - 2020 - tBERT Topic Models and BERT Joining Forces for Se.pdf},
  language = {en}
}

@article{piresHowMultilingualMultilingual2019,
  title = {How Multilingual Is {{Multilingual BERT}}?},
  author = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  year = {2019},
  month = jun,
  abstract = {In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2019) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.},
  archivePrefix = {arXiv},
  eprint = {1906.01502},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Pires et al. - 2019 - How multilingual is Multilingual BERT.pdf},
  journal = {arXiv:1906.01502 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@misc{POSTStatusesFilter,
  title = {{{POST}} Statuses/Filter},
  file = {/home/olivier/Zotero/storage/AUZLVEH4/post-statuses-filter.html},
  howpublished = {https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter},
  language = {en}
}

@inproceedings{rehurek_lrec,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r{u}}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  month = may,
  pages = {45--50},
  publisher = {{ELRA}},
  address = {{Valletta, Malta}},
  language = {English}
}

@article{reimers-2020-multilingual-sentence-bert,
  title = {Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2020},
  month = apr,
  archivePrefix = {arXiv},
  eprint = {2004.09813},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:2004.09813}
}

@inproceedings{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  pages = {3980--3990},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1410},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.},
  file = {/home/olivier/Zotero/storage/B2EHURDX/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf},
  language = {en}
}

@article{robnik-sikonjaCrosslingualTransferTwitter2020,
  title = {Cross-Lingual {{Transfer}} of {{Twitter Sentiment Models Using}} a {{Common Vector Space}}},
  author = {{Robnik-Sikonja}, Marko and Reba, Kristjan and Mozetic, Igor},
  year = {2020},
  month = may,
  abstract = {Word embeddings represent words in a numeric space in such a way that semantic relations between words are encoded as distances and directions in the vector space. Cross-lingual word embeddings map words from one language to the vector space of another language, or words from multiple languages to the same vector space where similar words are aligned. Cross-lingual embeddings can be used to transfer machine learning models between languages and thereby compensate for insufficient data in less-resourced languages. We use cross-lingual word embeddings to transfer machine learning prediction models for Twitter sentiment between 13 languages. We focus on two transfer mechanisms using the joint numerical space for many languages as implemented in the LASER library: the transfer of trained models, and expansion of training sets with instances from other languages. Our experiments show that the transfer of models between similar languages is sensible, while dataset expansion did not increase the predictive performance.},
  archivePrefix = {arXiv},
  eprint = {2005.07456},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Robnik-Sikonja et al. - 2020 - Cross-lingual Transfer of Twitter Sentiment Models.pdf},
  journal = {arXiv:2005.07456 [cs]},
  keywords = {68T50 (Primary),Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7,J.4,K.4.2},
  language = {en},
  primaryClass = {cs}
}

@article{ruderSurveyCrosslingualWord2019,
  title = {A {{Survey}} of {{Cross}}-Lingual {{Word Embedding Models}}},
  author = {Ruder, Sebastian and Vuli{\'c}, Ivan and S{\o}gaard, Anders},
  year = {2019},
  month = aug,
  volume = {65},
  pages = {569--631},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11640},
  abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
  file = {/home/olivier/nextcloud/zotero/Ruder et al. - 2019 - A Survey of Cross-lingual Word Embedding Models.pdf},
  journal = {Journal of Artificial Intelligence Research},
  language = {en}
}

@article{rupankabhuyanSurveyDensityBased2013,
  title = {A {{Survey}} of {{Some Density Based Clustering Techniques}}},
  author = {Rupanka Bhuyan and Samarjeet Borah},
  year = {2013},
  publisher = {{Unpublished}},
  doi = {10.13140/2.1.4554.6887},
  abstract = {Density Based Clustering are a type of Clustering methods using in data mining for extracting previously unknown patterns from data sets. There are a number of density based clustering methods such as DBSCAN, OPTICS, DENCLUE, VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of these methods is done along with their characteristics, advantages and disadvantages and most importantly \textendash{} their applicability to different types of data sets to mine useful and appropriate patterns.},
  file = {/home/olivier/nextcloud/zotero/Rupanka Bhuyan and Samarjeet Borah - 2013 - A Survey of Some Density Based Clustering Techniqu.pdf},
  language = {en}
}

@misc{StandardSearchAPI,
  title = {Standard Search {{API}}},
  file = {/home/olivier/Zotero/storage/QEAFYHBM/get-search-tweets.html},
  howpublished = {https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets},
  language = {en}
}

@misc{TweetObject,
  title = {Tweet Object},
  file = {/home/olivier/Zotero/storage/ACRCC2T2/tweet-object.html},
  howpublished = {https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object},
  language = {en}
}

@article{twinandillaMultiDocumentSummarizationUsing2018,
  ids = {twinandillaMultiDocumentSummarizationUsing2018a},
  title = {Multi-{{Document Summarization Using K}}-{{Means}} and {{Latent Dirichlet Allocation}} ({{LDA}}) \textendash{} {{Significance Sentences}}},
  author = {Twinandilla, Shiva and Adhy, Satriyo and Surarso, Bayu and Kusumaningrum, Retno},
  year = {2018},
  month = jan,
  volume = {135},
  pages = {663--670},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2018.08.220},
  abstract = {The contents of online news documents are almost the same that will lead to the redundancy of news or called yellow journalism. Yellow journalism can make it difficult for readers to distinguish documents containing fact or opinionated information. Therefore, it is necessary to extend more research about multi-document summarization so that readers can easily understand the intent of online news documents. Latent Dirichlet Allocation (LDA) - Significance Sentences is one of the methods for summarization, which performs better than the term frequency algorithm. However, document summarization using the method is only able to summarize multiple documents as a whole without grouping by topic. Subsequently, it can give an unorganized summary result. Therefore, this research proposes a novel summarization method which combines K-Means Clustering and LDA - Significance Sentences, so it can generate document summaries based on the topic. We implemented two scenarios of the experiment. The first experimental results the best alpha value is 0.001 with the ROUGE-1 value of 0.5545 and the best summarization level is 30\% with the ROUGE-1 value of 0.6118. While the second experiment results, the best obtain of ROUGE-1 value is 0.61991 for the first cluster which is consists of documents 1, 2, 3, 4, and 6 and 0.6139 for the second cluster which is consists of 5, 7, 8. Multi-document summarization using the proposed method has good performance when the K-Means method can cluster the document according to the topic correctly, which is highly dependent on the accuracy of determining the initial centroid.},
  file = {/home/olivier/nextcloud/zotero/Twinandilla et al. - 2018 - Multi-Document Summarization Using K-Means and Lat.pdf},
  journal = {Procedia Computer Science},
  keywords = {K-Means,Latent Dirichlet Allocation,Multi-Document Summarization,Online News,ROUGE-1,Significance Sentences,Yellow Journalism},
  language = {en},
  series = {The 3rd {{International Conference}} on {{Computer Science}} and {{Computational Intelligence}} ({{ICCSCI}} 2018) : {{Empowering Smart Technology}} in {{Digital Era}} for a {{Better Life}}}
}

@article{van2014accelerating,
  title = {Accelerating T-{{SNE}} Using Tree-Based Algorithms},
  author = {Van Der Maaten, Laurens},
  year = {2014},
  volume = {15},
  pages = {3221--3245},
  publisher = {{JMLR. org}},
  file = {/home/olivier/nextcloud/zotero/Van Der Maaten - 2014 - Accelerating t-SNE using tree-based algorithms.pdf},
  journal = {The Journal of Machine Learning Research},
  number = {1}
}

@article{veerannaUsingSemanticSimilarity2016,
  title = {Using {{Semantic Similarity}} for {{Multi}}-{{Label Zero}}-{{Shot Classification}} of {{Text Documents}}},
  author = {Veeranna, Sappadla Prateek and Nam, Jinseok and Menc{\i}a, Eneldo Loza and Furnkranz, Johannes},
  year = {2016},
  pages = {6},
  abstract = {In this paper, we examine a simple approach to zero-shot multi-label text classification, i.e., to the problem of predicting multiple, possibly previously unseen labels for a document. In particular, we propose to use a semantic embedding of label and document words and base the prediction of previously unseen labels on the similarity between the label name and the document words in this embedding. Experiments on three textual datasets across various domains show that even such a simple technique yields considerable performance improvements over a simple uninformed baseline.},
  file = {/home/olivier/nextcloud/zotero/Veeranna et al. - 2016 - Using Semantic Similarity for Multi-Label Zero-Sho.pdf},
  journal = {Computational Intelligence},
  language = {en}
}

@article{Wolf2019HuggingFacesTS,
  title = {{{HuggingFace}}'s Transformers: {{State}}-of-the-Art Natural Language Processing},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2019},
  volume = {abs/1910.03771},
  journal = {ArXiv}
}

@article{xiao2018bertservice,
  title = {Bert-as-Service},
  author = {Xiao, Han},
  year = {2018}
}

@article{xieMonolingualMultilingualTopic2020,
  title = {Monolingual and Multilingual Topic Analysis Using {{LDA}} and {{BERT}} Embeddings},
  author = {Xie, Qing and Zhang, Xinyuan and Ding, Ying and Song, Min},
  year = {2020},
  month = aug,
  volume = {14},
  pages = {101055},
  issn = {17511577},
  doi = {10.1016/j.joi.2020.101055},
  abstract = {Analyzing research topics offers potential insights into the direction of scientific development. In particular, analyzing multilingual research topics can help researchers grasp the evolution of topics globally, revealing topic similarity among scientific publications written in different languages. Most studies to date on topic analysis have been based on Englishlanguage publications and have relied heavily on citation-based topic evolution analysis. However, since it can be challenging for English publications to cite non-English sources and since many languages do not offer English translations of abstracts, citation-based methodologies are not suitable for analyzing multilingual research topic relations. Since multilingual sentence embeddings can effectively preserve word semantics in multilingual translation tasks, a topic model based on multilingual sentence embeddings could potentially generate topic\textendash word distributions for publications in multilingual analysis. In this paper, which is situated in the field of library and information science, we use multilingual pretrained Bidirectional Encoder Representations from Transformers (BERT) embeddings and the Latent Dirichlet Allocation (LDA) topic model to analyze topic evolution in monolingual and multilingual topic similarity settings. For each topic, we multiply its LDA probability value by the averaged tensor similarity of BERT embeddings to explore the evolution of the topic in scientific publications. As our proposed method does not rely on a machine translator or the author's subjective translation, it avoids confusion and misusages caused by either machine error or the author's subjectively chosen English keywords. Our results show that the proposed approach is well-suited to analyzing the scientific evolutions in monolingual and scientific multilingual topic similarity relations.},
  file = {/home/olivier/Zotero/storage/23RWPIAA/Xie et al. - 2020 - Monolingual and multilingual topic analysis using .pdf},
  journal = {Journal of Informetrics},
  language = {en},
  number = {3}
}

@article{yangMultilingualUniversalSentence2019,
  title = {Multilingual {{Universal Sentence Encoder}} for {{Semantic Retrieval}}},
  author = {Yang, Yinfei and Cer, Daniel and Ahmad, Amin and Guo, Mandy and Law, Jax and Constant, Noah and Abrego, Gustavo Hernandez and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2019},
  month = jul,
  abstract = {We introduce two pre-trained retrieval focused multilingual sentence encoding models, respectively based on the Transformer and CNN model architectures. The models embed text from 16 languages into a single semantic space using a multi-task trained dualencoder that learns tied representations using translation based bridge tasks (Chidambaram et al., 2018). The models provide performance that is competitive with the state-ofthe-art on: semantic retrieval (SR), translation pair bitext retrieval (BR) and retrieval question answering (ReQA). On English transfer learning tasks, our sentence-level embeddings approach, and in some cases exceed, the performance of monolingual, English only, sentence embedding models. Our models are made available for download on TensorFlow Hub.},
  archivePrefix = {arXiv},
  eprint = {1907.04307},
  eprinttype = {arxiv},
  file = {/home/olivier/nextcloud/zotero/Yang et al. - 2019 - Multilingual Universal Sentence Encoder for Semant.pdf},
  journal = {arXiv:1907.04307 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}


